## Adversarial-example-security
### A demo for 'How secure are the adversarial examples themselves ?' 
    submission to The 28th International Conference on Neural Information Processing (ICONIP2021)
    
See 'Security of adversarial examples-supplementary.pdf' for more results. The demo shows how the proposed two-step test works. Test1 utilyzes the instability nature of adversarial examples, whereas test2 is based on the fact that adversarial perturbation destroys local correlation in natural images.

### Usage
For test1, run Test1_AddDe.py in the /Test1 folder  
For test2, run Demo_test2.m 



    
